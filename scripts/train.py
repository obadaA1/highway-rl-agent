"""
Main Training Script for Highway RL Agent.

Purpose:
    Train PPO agent for 200k timesteps with checkpointing and logging.
    
Outputs:
    - Checkpoints at 0k, 100k, 200k (for evolution video)
    - TensorBoard logs (for training analysis plots)
    - Training progress updates every 10k steps

Compliance:
    - Type hints everywhere
    - No magic numbers (uses config.py)
    - Reproducible (fixed seed from config)
    - Follows rubric requirements

Run: python scripts/train.py
Expected time: ~1.5 hours (40 vehicles @ 40-45 it/s)
"""

import sys
import argparse
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.env.highway_env_v6 import make_highway_env_v6
from src.agent.ppo_agent import HighwayPPOAgent
from src.training.callbacks import (
    CheckpointCallback,
    CustomMetricsCallback,
    ProgressCallback,
)
from src.config import TRAINING_CONFIG, CHECKPOINT_CONFIG
from stable_baselines3.common.callbacks import CallbackList
from stable_baselines3 import PPO


def main() -> None:
    """
    Execute training run with resume support.
    
    Configuration:
        - Total timesteps: 200,000 (thorough training for 40 vehicles)
        - Checkpoint frequency: 100,000 steps
        - Saves at: 0k, 100k, 200k (for evolution video requirement)
        - Progress updates: Every 10,000 steps
        - Vehicle count: 40 (dense traffic)
        - Reward: Multi-objective (speed vs safety) with weaving detection
    
    Resume Support:
        python scripts/train.py --resume path/to/checkpoint.zip --additional-steps 100000
    
    Evolution Video Requirement:
        This script produces the 3 checkpoints needed for the rubric:
        1. assets/checkpoints/highway_ppo_0_steps.zip (untrained)
        2. assets/checkpoints/highway_ppo_100000_steps.zip (half-trained)
        3. assets/checkpoints/highway_ppo_200000_steps.zip (fully-trained)
    """
    # Parse command-line arguments
    parser = argparse.ArgumentParser(description="Train Highway RL Agent (V6 Multi-Objective)")
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="Path to checkpoint to resume from (e.g., assets/checkpoints/highway_ppo_100000_steps.zip)"
    )
    parser.add_argument(
        "--additional-steps",
        type=int,
        default=None,
        help="Additional steps to train from checkpoint (if not specified, uses config)"
    )
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("HIGHWAY RL AGENT - TRAINING (V6: Multi-Objective Safe Driving)")
    print("="*70)
    
    # 1. Create V6 environment
    print("\nüì¶ Creating V6 Multi-Objective environment...")
    env = make_highway_env_v6(render_mode=None)
    print("‚úÖ Environment ready")
    
    # 2. Determine if resuming or starting fresh
    resume_mode = args.resume is not None
    starting_timesteps = 0
    
    if resume_mode:
        print(f"\nüìÇ RESUME MODE: Loading checkpoint...")
        print(f"   Checkpoint: {args.resume}")
        
        # Load existing model
        try:
            model = PPO.load(args.resume, env=env)
            starting_timesteps = model.num_timesteps
            print(f"‚úÖ Checkpoint loaded successfully")
            print(f"   Current progress: {starting_timesteps:,} steps completed")
        except Exception as e:
            print(f"‚ùå Failed to load checkpoint: {e}")
            print(f"   Starting fresh training instead...")
            resume_mode = False
    
    if not resume_mode:
        print(f"\nüÜï FRESH TRAINING: Creating new agent...")
        starting_timesteps = 0
    
    # Determine training steps
    if args.additional_steps is not None:
        additional_steps = args.additional_steps
    elif resume_mode:
        # Default: train to 200k total
        additional_steps = TRAINING_CONFIG['total_timesteps'] - starting_timesteps
    else:
        additional_steps = TRAINING_CONFIG['total_timesteps']
    
    target_timesteps = starting_timesteps + additional_steps
    
    print(f"\nüìä Training Configuration:")
    print(f"   Starting from: {starting_timesteps:,} steps")
    print(f"   Training for: {additional_steps:,} additional steps")
    print(f"   Target: {target_timesteps:,} steps")
    print(f"   Checkpoint frequency: {CHECKPOINT_CONFIG['save_freq']:,} steps")
    print(f"   Vehicle count: 40 (dense traffic)")
    print(f"   Policy frequency: 12 Hz (83ms reactions)")
    print(f"   Device: {TRAINING_CONFIG.get('device', 'auto')}")
    print(f"   Seed: {TRAINING_CONFIG.get('seed', 42)}")
    
    print(f"\nüéØ V6 Multi-Objective Reward (Rubric-Compliant):")
    print(f"   R(s,a) = R_speed + R_safe_distance - P_weaving - P_slow - P_collision")
    print(f"   ‚úì R_speed: High velocity (speed objective)")
    print(f"   ‚úì R_safe_distance: Safe following when car ahead (safety objective)")
    print(f"   ‚úì P_weaving: Penalize consecutive lane changes only (unnecessary)")
    print(f"   ‚úì P_slow: Penalize driving <60% speed")
    print(f"   ‚úì P_collision: Crash penalty at termination")
    print(f"\nüí° Key Design:")
    print(f"   - Strategic lane changes (passing) = NO penalty")
    print(f"   - Weaving (consecutive <10 steps) = penalized")
    print(f"   - Balances speed vs safety (multi-objective)")
    print("="*70 + "\n")
    
    # 3. Initialize agent (or use loaded model)
    if not resume_mode:
        print("ü§ñ Initializing new PPO agent...")
        agent = HighwayPPOAgent(
            env=env,
            learning_rate=TRAINING_CONFIG["learning_rate"],
            n_steps=TRAINING_CONFIG["n_steps"],
            batch_size=TRAINING_CONFIG["batch_size"],
            n_epochs=TRAINING_CONFIG["n_epochs"],
            gamma=TRAINING_CONFIG["gamma"],
            gae_lambda=TRAINING_CONFIG["gae_lambda"],
            clip_range=TRAINING_CONFIG["clip_range"],
            ent_coef=TRAINING_CONFIG["ent_coef"],
            device=TRAINING_CONFIG.get("device", "auto"),
            seed=TRAINING_CONFIG.get("seed", 42),
            verbose=1,
        )
        print("‚úÖ New agent ready")
    else:
        # Wrap loaded PPO model in HighwayPPOAgent interface
        agent = HighwayPPOAgent(
            env=env,
            learning_rate=TRAINING_CONFIG["learning_rate"],
            n_steps=TRAINING_CONFIG["n_steps"],
            batch_size=TRAINING_CONFIG["batch_size"],
            n_epochs=TRAINING_CONFIG["n_epochs"],
            gamma=TRAINING_CONFIG["gamma"],
            gae_lambda=TRAINING_CONFIG["gae_lambda"],
            clip_range=TRAINING_CONFIG["clip_range"],
            ent_coef=TRAINING_CONFIG["ent_coef"],
            device=TRAINING_CONFIG.get("device", "auto"),
            seed=TRAINING_CONFIG.get("seed", 42),
            verbose=1,
        )
        # Replace with loaded model
        agent.model = model
        print("‚úÖ Resumed agent ready")
    
    # 4. Setup callbacks
    print("\n‚öôÔ∏è  Configuring callbacks...")
    
    # Checkpoint callback (CRITICAL for evolution video)
    checkpoint_callback = CheckpointCallback(
        checkpoint_dir=CHECKPOINT_CONFIG["save_path"],
        save_freq=CHECKPOINT_CONFIG["save_freq"],
        name_prefix=CHECKPOINT_CONFIG["name_prefix"],
        verbose=1,
    )
    
    # Metrics callback (for TensorBoard analysis)
    metrics_callback = CustomMetricsCallback(
        log_freq=TRAINING_CONFIG.get("log_freq", 1000),
        verbose=1,
    )
    
    # Progress callback (for ETA and speed monitoring)
    # Pass target_timesteps so progress shows correctly for resume
    progress_callback = ProgressCallback(
        total_timesteps=target_timesteps,
        starting_timesteps=starting_timesteps,  # New parameter for resume
        update_freq=10_000,  # Update every 10k steps
        verbose=1,
    )
    
    # Combine callbacks
    callback = CallbackList([
        checkpoint_callback,
        metrics_callback,
        progress_callback,
    ])
    
    print("‚úÖ Callbacks ready")
    
    # 5. Start training
    mode_str = "RESUMING" if resume_mode else "STARTING"
    print(f"\nüöÄ {mode_str} training...")
    print("="*70)
    estimated_hours = additional_steps / 40000  # Rough estimate at 40k steps/hour
    print(f"‚è∞ Estimated time: ~{estimated_hours:.1f} hours")
    print(f"üìä Monitor progress: tensorboard --logdir=tensorboard_logs")
    print(f"‚ö†Ô∏è  Press Ctrl+C to pause (checkpoint will be saved)")
    print("="*70 + "\n")
    
    try:
        agent.train(
            total_timesteps=additional_steps,  # Train for additional steps only
            callback=callback,
            tb_log_name="highway_ppo_training",
            reset_num_timesteps=False,  # CRITICAL: Don't reset counter for resume!
        )
        
        print("\n" + "="*70)
        print("‚úÖ TRAINING COMPLETE! (V6: Multi-Objective)")
        print("="*70)
        print("\nGenerated artifacts:")
        print(f"  üìÅ Checkpoints: {CHECKPOINT_CONFIG['save_path']}/")
        print(f"     - highway_ppo_0_steps.zip (untrained)")
        print(f"     - highway_ppo_100000_steps.zip (half-trained)")
        print(f"     - highway_ppo_200000_steps.zip (fully-trained)")
        print(f"  üìä TensorBoard logs: tensorboard_logs/highway_ppo_training_*/")
        print("\nV6 Multi-Objective Success Criteria:")
        print("  ‚úì High speed maintained (R_speed objective)")
        print("  ‚úì Safe distance bonuses earned (R_safe_distance objective)")
        print("  ‚úì Strategic lane changes (passing allowed)")
        print("  ‚úì No excessive weaving (consecutive changes penalized)")
        print("  ‚úì Crash rate improved over V5")
        print("  ‚úì Multi-objective balance: speed vs safety")
        print("\nNext steps:")
        print("  1. Evaluate: python scripts/evaluate.py")
        print("  2. Record videos: python scripts/record_video.py")
        print("  3. Analyze: tensorboard --logdir tensorboard_logs")
        print("="*70 + "\n")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Training interrupted by user (Ctrl+C)")
        print("üíæ Saving interrupt checkpoint...")
        
        # Save current state
        current_steps = agent.model.num_timesteps
        interrupt_path = Path(CHECKPOINT_CONFIG['save_path']) / f"highway_ppo_{current_steps}_steps_interrupted.zip"
        agent.save(interrupt_path)
        
        print(f"‚úÖ Checkpoint saved: {interrupt_path}")
        print(f"üìù To resume: python scripts/train.py --resume {interrupt_path}")
    except Exception as e:
        print(f"\n\n‚ùå Training failed with error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        env.close()
        print("\nüîí Environment closed")


if __name__ == "__main__":
    main()
